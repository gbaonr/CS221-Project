{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12308497,"sourceType":"datasetVersion","datasetId":7758166},{"sourceId":451875,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":366554,"modelId":387455},{"sourceId":455175,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":369154,"modelId":390029}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install seqeval -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:23:55.743719Z","iopub.execute_input":"2025-07-01T14:23:55.744058Z","iopub.status.idle":"2025-07-01T14:24:03.129948Z","shell.execute_reply.started":"2025-07-01T14:23:55.744033Z","shell.execute_reply":"2025-07-01T14:24:03.129017Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\nfrom torch.utils.data import Dataset\nimport torch\nimport os\n\n# 1. Load d·ªØ li·ªáu v√† x·ª≠ l√Ω NaN\ndef load_data(path):\n    df = pd.read_excel(path)\n    grouped = df.groupby(\"index\")\n    sentences = grouped[\"tokens\"].apply(list).tolist()\n    labels = grouped[\"tags\"].apply(list).tolist()\n    return sentences[1:], labels[1:]  # b·ªè d√≤ng ƒë·∫ßu n·∫øu c·∫ßn\ndata_path = \"/kaggle/input/relabel-cs221\"\n\nsentences, labels = None, None\nfor f in os.listdir(data_path):\n    f_path = os.path.join(data_path, f)\n    if not sentences or not labels:\n        sentences, labels = load_data(f_path)\n    sents, labs = load_data(f_path)\n    sentences += sents\n    labels += labs\n\n# sentences, labels = load_data(\"/kaggle/input/relabel-cs221/val_500_0.xlsx\")\n# sents2, labels2 = load_data(\"/kaggle/input/relabel-cs221/test_500_0.xlsx\")\n# sents3, labels3 = load_data(\"/kaggle/input/relabel-cs221/val_500_vert_1.xlsx\")\n# sents4, labels4 = load_data(\"/kaggle/input/relabel-cs221/test_500_vert_1.xlsx\")\n\n# sentences += sents2 + sents3 + sents4\n# labels += labels2 + labels3 + labels4\n\n\ndef contains_nan(seq):\n    return any(pd.isna(token) for token in seq)\n\nfiltered_sentences, filtered_labels = [], []\nfor tokens, tags in zip(sentences, labels):\n    if not contains_nan(tokens) and not contains_nan(tags):\n        filtered_sentences.append(tokens)\n        filtered_labels.append(tags)\n\nsentences = filtered_sentences\nlabels = filtered_labels\n\n# 2. Chu·∫©n b·ªã tokenizer v√† Dataset\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\nlabel_list = [\"O\", \"B-PER\", \"I-PER\",  \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\nlabel2id = {label: i for i, label in enumerate(label_list)}\nid2label = {i: label for label, i in label2id.items()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:23:24.308551Z","iopub.execute_input":"2025-07-01T14:23:24.309150Z","iopub.status.idle":"2025-07-01T14:23:25.780422Z","shell.execute_reply.started":"2025-07-01T14:23:24.309124Z","shell.execute_reply":"2025-07-01T14:23:25.779614Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport torch\n\nclass NERDataset(Dataset):\n    def __init__(self, sentences, labels, tokenizer, max_len=128):\n        self.sentences = sentences\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx):\n        words = self.sentences[idx]\n        word_labels = self.labels[idx]\n\n        tokens = []\n        label_ids = []\n\n        for word, label in zip(words, word_labels):\n            word_tokens = self.tokenizer.tokenize(word)\n            tokens.extend(word_tokens)\n            label_ids.extend([label] + [-100] * (len(word_tokens) - 1))\n\n        tokens = [self.tokenizer.cls_token] + tokens[:self.max_len - 2] + [self.tokenizer.sep_token]\n        label_ids = [-100] + label_ids[:self.max_len - 2] + [-100]\n\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        attention_mask = [1] * len(input_ids)\n\n        pad_len = self.max_len - len(input_ids)\n        input_ids += [self.tokenizer.pad_token_id] * pad_len\n        attention_mask += [0] * pad_len\n        label_ids += [-100] * pad_len\n\n        return {\n            'input_ids': torch.tensor(input_ids),\n            'attention_mask': torch.tensor(attention_mask),\n            'labels': torch.tensor(label_ids)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:23:25.781151Z","iopub.execute_input":"2025-07-01T14:23:25.781418Z","iopub.status.idle":"2025-07-01T14:23:25.788825Z","shell.execute_reply.started":"2025-07-01T14:23:25.781392Z","shell.execute_reply":"2025-07-01T14:23:25.787872Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from seqeval.metrics import accuracy_score, precision_score, recall_score, f1_score\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    true_labels = []\n    true_preds = []\n\n    for pred, label in zip(predictions, labels):\n        temp_labels = []\n        temp_preds = []\n        for p_, l_ in zip(pred, label):\n            if l_ != -100:\n                temp_labels.append(id2label[l_])\n                temp_preds.append(id2label[p_])\n        true_labels.append(temp_labels)\n        true_preds.append(temp_preds)\n\n    return {\n        \"precision\": precision_score(true_labels, true_preds),\n        \"recall\": recall_score(true_labels, true_preds),\n        \"f1\": f1_score(true_labels, true_preds),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:24:03.131636Z","iopub.execute_input":"2025-07-01T14:24:03.131905Z","iopub.status.idle":"2025-07-01T14:24:03.148720Z","shell.execute_reply.started":"2025-07-01T14:24:03.131880Z","shell.execute_reply":"2025-07-01T14:24:03.148013Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n\nmodel_path = \"/kaggle/input/phobert_ner_cs221/other/default/1/phobert-ner/checkpoint-3125\"\n\nimport json\nwith open(f\"{model_path}/config.json\") as f:\n    checkpoint_config = json.load(f)\n\nprint(\"Label mapping t·ª´ checkpoint:\", checkpoint_config[\"id2label\"])\n\n# 3. S·ª≠ d·ª•ng label mapping t·ª´ checkpoint thay v√¨ ƒë·ªãnh nghƒ©a m·ªõi\nid2label = {int(k): v for k, v in checkpoint_config[\"id2label\"].items()}\nlabel2id = {v: k for k, v in id2label.items()}\nlabel_list = list(label2id.keys())\n\nmodel = AutoModelForTokenClassification.from_pretrained(\n    # \"vinai/phobert-base\",\n    model_path,\n    num_labels=len(label_list),\n    id2label=id2label,\n    label2id=label2id\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n\n\nsentences = [[str(w) for w in sent] for sent in sentences]\n\ntrain_dataset = NERDataset(sentences, labels, tokenizer)\n\n\n\nargs = TrainingArguments(\n    output_dir=\"./phobert-ner\",\n    per_device_train_batch_size=16,\n    num_train_epochs=20,           # T·ªïng s·ªë epoch mong mu·ªën\n    logging_dir=\"./logs\",         # üëà ƒë·ªÉ Trainer log ƒë√∫ng\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    report_to=\"none\",\n    save_total_limit=1,\n)\n\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n\n)\n\n# Fine-tune ti·∫øp\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:24:04.066587Z","iopub.execute_input":"2025-07-01T14:24:04.066902Z","iopub.status.idle":"2025-07-01T14:37:27.259726Z","shell.execute_reply.started":"2025-07-01T14:24:04.066878Z","shell.execute_reply":"2025-07-01T14:37:27.258835Z"}},"outputs":[{"name":"stdout","text":"Label mapping t·ª´ checkpoint: {'0': 'O', '1': 'B-PER', '2': 'I-PER', '3': 'B-ORG', '4': 'I-ORG', '5': 'B-LOC', '6': 'I-LOC'}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/270240647.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1520' max='1520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1520/1520 13:14, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.351000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.106400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.078100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.040400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.027500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.021400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.014700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.011600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.012200</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.006100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.004200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1520, training_loss=0.04563206637180165, metrics={'train_runtime': 797.1528, 'train_samples_per_second': 60.766, 'train_steps_per_second': 1.907, 'total_flos': 3164446765455360.0, 'train_loss': 0.04563206637180165, 'epoch': 20.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ntest_sentences = [\n    [\"Th√†nh\", \"ph·ªë\", \"H·ªì\", \"Ch√≠\", \"Minh\", \"l√†\", \"trung\", \"t√¢m\", \"kinh\", \"t·∫ø\", \"l·ªõn\", \"nh·∫•t\", \"Vi·ªát\", \"Nam\", \".\"],\n    [\"Ti·∫øn\", \"sƒ©\", \"L√™\", \"Th·ªã\", \"Thu\", \"H√†\", \"ƒëang\", \"nghi√™n\", \"c·ª©u\", \"v·ªÅ\", \"tr√≠\", \"tu·ªá\", \"nh√¢n\", \"t·∫°o\", \".\"],\n    [\"T·∫≠p\", \"ƒëo√†n\", \"FPT\", \"Software\", \"l√†\", \"m·ªôt\", \"trong\", \"nh·ªØng\", \"c√¥ng\", \"ty\", \"c√¥ng\", \"ngh·ªá\", \"h√†ng\", \"ƒë·∫ßu\", \"Vi·ªát\", \"Nam\", \".\"],\n    [\"V·ªãnh\", \"H·∫°\", \"Long\", \"ƒë∆∞·ª£c\", \"UNESCO\", \"c√¥ng\", \"nh·∫≠n\", \"l√†\", \"Di\", \"s·∫£n\", \"Thi√™n\", \"nhi√™n\", \"Th·∫ø\", \"gi·ªõi\", \".\"],\n    [\"C√¥ng\", \"ty\", \"c·ªï\", \"ph·∫ßn\", \"S·ªØa\", \"Vi·ªát\", \"Nam\", \"Vinamilk\", \"l√†\", \"nh√†\", \"s·∫£n\", \"xu·∫•t\", \"s·ªØa\", \"h√†ng\", \"ƒë·∫ßu\", \"Vi·ªát\", \"Nam\", \".\"],\n    [\"S√¥ng\", \"H·ªìng\", \"ch·∫£y\", \"qua\", \"th·ªß\", \"ƒë√¥\", \"H√†\", \"N·ªôi\", \"c·ªßa\", \"Vi·ªát\", \"Nam\", \".\"],\n    [\"√îng\", \"Nguy·ªÖn\", \"Thanh\", \"Long\", \"t·ª´ng\", \"l√†\", \"B·ªô\", \"tr∆∞·ªüng\", \"B·ªô\", \"Y\", \"t·∫ø\", \".\"],\n    [\"C·∫ßu\", \"C·∫ßn\", \"Th∆°\", \"n·ªëi\", \"li·ªÅn\", \"hai\", \"b·ªù\", \"s√¥ng\", \"H·∫≠u\", \"thu·ªôc\", \"th√†nh\", \"ph·ªë\", \"C·∫ßn\", \"Th∆°\", \".\"],\n    [\"T·ªïng\", \"B√≠\", \"th∆∞\", \"T√¥\", \"L√¢m\", \"ƒë√£\", \"c√≥\", \"b√†i\", \"ph√°t\", \"bi·ªÉu\", \"quan\", \"tr·ªçng\", \".\"],\n    [\"Li√™n\", \"H·ª£p\", \"Qu·ªëc\", \"c√≥\", \"tr·ª•\", \"s·ªü\", \"ch√≠nh\", \"t·∫°i\", \"New\", \"York\", \",\", \"Hoa\", \"K·ª≥\", \".\"],\n    ['ƒë·ªïi', 'Fukui', '(', 'th√†nh', 'ph·ªë', ')'],\n    ['Con', 'ƒë∆∞·ªùng', 't∆°', 'l·ª•a'],\n    ['ƒë·ªïi', 'Ch·ªâ', 'th·ªã', 'v·ªÅ', 'h·∫°n', 'ch·∫ø', 'c√°c', 'ch·∫•t', 'nguy', 'hi·ªÉm'],\n    ['T·ªïng', 'th·ªëng', 'Hoa', 'K·ª≥']\n]\ntoken_list = []\nlabel_list = []\n\nfor test_sentence in test_sentences:\n    model.eval()\n    encoding = tokenizer(test_sentence, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n    encoding = {k: v.to(device) for k, v in encoding.items()}\n    with torch.no_grad():\n        output = model(**encoding)\n    \n    pred_ids = output.logits.argmax(dim=-1).squeeze().tolist()\n    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())\n    \n    # Map index v·ªÅ nh√£n\n    pred_labels = [model.config.id2label[idx] for idx in pred_ids]\n    token_list.append(tokens)\n    label_list.append(pred_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:43:05.070778Z","iopub.execute_input":"2025-07-01T14:43:05.071375Z","iopub.status.idle":"2025-07-01T14:43:05.219525Z","shell.execute_reply.started":"2025-07-01T14:43:05.071332Z","shell.execute_reply":"2025-07-01T14:43:05.218745Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# **Model ch∆∞a finetune predict**","metadata":{}},{"cell_type":"code","source":"model_ = AutoModelForTokenClassification.from_pretrained(\n    \"/kaggle/input/phobert_ner_cs221/other/default/1/phobert-ner/checkpoint-3125\", \n    num_labels=7\n)\n\nlabel_list_ = []\n\n\nfor test_sentence in test_sentences:\n    model_.eval()\n    encoding = tokenizer(test_sentence, is_split_into_words=True, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        output = model_(**encoding)\n    \n    pred_ids = output.logits.argmax(dim=-1).squeeze().tolist()\n    tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze())\n    \n    # Map index v·ªÅ nh√£n\n    pred_labels = [model.config.id2label[idx] for idx in pred_ids]\n    label_list_.append(pred_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:43:07.389780Z","iopub.execute_input":"2025-07-01T14:43:07.390102Z","iopub.status.idle":"2025-07-01T14:43:08.403642Z","shell.execute_reply.started":"2025-07-01T14:43:07.390072Z","shell.execute_reply":"2025-07-01T14:43:08.402953Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# connect separated tokens\ndef connect_subwords(token_list, labels1, labels2):\n    token_list_new, labels1_new, labels2_new = [], [], []\n    for i, tokens in enumerate(token_list):\n        t_tokens, t_l1, t_l2 = [], [], []\n        flag = 0\n        for token, l1, l2 in zip(token_list[i], labels1[i], labels2[i]):\n            if token == \"<s>\" or token == \"</s>\":\n                continue\n            if flag == 1:\n                flag = 0\n                token = token.replace(\"@\", \"\")\n                t_tokens[-1] = t_tokens[-1] + token\n                print(t_tokens[-1])\n                continue\n            if \"@\" in token and flag == 0:\n                flag = 1\n                token = token.replace(\"@\", \"\")\n            \n            t_tokens.append(token)\n            t_l1.append(l1)\n            t_l2.append(l2)\n        token_list_new.append(t_tokens)\n        labels1_new.append(t_l1)\n        labels2_new.append(t_l2)\n    return token_list_new, labels1_new, labels2_new\n\ntoken_list, label_list, label_list_ = connect_subwords(token_list, label_list, label_list_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:43:10.080614Z","iopub.execute_input":"2025-07-01T14:43:10.081213Z","iopub.status.idle":"2025-07-01T14:43:10.088109Z","shell.execute_reply.started":"2025-07-01T14:43:10.081188Z","shell.execute_reply":"2025-07-01T14:43:10.087336Z"}},"outputs":[{"name":"stdout","text":"tu·ªá\nFukui\nth·ªëng\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"for i, token in enumerate(token_list):\n    if i not in [4, 8, 11]:\n        continue\n    for token, label, label_, in zip(token_list[i], label_list[i], label_list_[i]):\n        print(f\"{token:10} {label:8} {label_:8}\")\n        # print(f\"{token:10} {label_:8}\")\n    print(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:43:13.370431Z","iopub.execute_input":"2025-07-01T14:43:13.370717Z","iopub.status.idle":"2025-07-01T14:43:13.376132Z","shell.execute_reply.started":"2025-07-01T14:43:13.370695Z","shell.execute_reply":"2025-07-01T14:43:13.375286Z"}},"outputs":[{"name":"stdout","text":"C√¥ng       B-ORG    O       \nty         I-ORG    O       \nc·ªï         I-ORG    I-ORG   \nph·∫ßn       I-ORG    I-ORG   \nS·ªØa        I-ORG    I-ORG   \nVi·ªát       I-ORG    I-ORG   \nNam        I-ORG    I-ORG   \nVinamilk   I-ORG    B-ORG   \nl√†         O        O       \nnh√†        O        O       \ns·∫£n        O        O       \nxu·∫•t       O        O       \ns·ªØa        O        O       \nh√†ng       O        O       \nƒë·∫ßu        O        O       \nVi·ªát       B-LOC    B-LOC   \nNam        I-LOC    I-LOC   \n.          O        O       \n==================================================\nT·ªïng       B-PER    B-ORG   \nB√≠         I-PER    I-ORG   \nth∆∞        I-PER    I-ORG   \nT√¥         I-PER    I-ORG   \nL√¢m        I-PER    I-ORG   \nƒë√£         O        O       \nc√≥         O        O       \nb√†i        O        O       \nph√°t       O        O       \nbi·ªÉu       O        O       \nquan       O        O       \ntr·ªçng      O        O       \n.          O        O       \n==================================================\nCon        B-LOC    B-ORG   \nƒë∆∞·ªùng      I-LOC    I-ORG   \nt∆°         I-LOC    I-ORG   \nl·ª•a        I-LOC    I-ORG   \n==================================================\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Print\nfor i, tokens in enumerate(token_list):\n    tokens_ = []\n    labels = []\n    labels_ = []\n    \n    for token, label, label_ in zip(token_list[i], label_list[i], label_list_[i]):\n        # label = label2id[label]\n        # label_ = label2id[label_]\n        pad = 6\n        if len(token) > 5:\n            pad = 7\n        \n        tokens_.append(f\"{token:{pad}}\")\n        labels.append(f\"{label:{pad}}\")\n        labels_.append(f\"{label_:{pad}}\")\n\n    print(\"Tokens:    \" + \" \".join(tokens_))\n    print(\"PhoBERT:   \" + \" \".join(labels_))\n    print(\"Finetuned: \" + \" \".join(labels))\n    print(\"=\" * 100)\n\noutput_lines = []\n\nfor i, tokens in enumerate(token_list):\n    tokens_ = []\n    labels = []\n    labels_ = []\n\n    for token, label, label_ in zip(token_list[i], label_list[i], label_list_[i]):\n        pad = 6 \n        if len(token) > 5:\n            pad = len(token)\n        \n        tokens_.append(f\"{token:{pad}}\")\n        labels.append(f\"{label:{pad}}\")\n        labels_.append(f\"{label_:{pad}}\")\n\n    line1 = \"Tokens:    \" + \" \".join(tokens_)\n    line2 = \"PhoBERT:   \" + \" \".join(labels_)\n    line3 = \"Finetuned: \" + \" \".join(labels)\n    separator = \"=\" * 100\n\n    output_lines.extend([line1, line2, line3, separator])\n\n# Ghi ra file\nwith open(\"ner_comparison.txt\", \"w\", encoding=\"utf-8\") as f:\n    for line in output_lines:\n        f.write(line + \"\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:43:36.094234Z","iopub.execute_input":"2025-07-01T14:43:36.094517Z","iopub.status.idle":"2025-07-01T14:43:36.104136Z","shell.execute_reply.started":"2025-07-01T14:43:36.094494Z","shell.execute_reply":"2025-07-01T14:43:36.103388Z"}},"outputs":[{"name":"stdout","text":"Tokens:    Th√†nh  ph·ªë    H·ªì     Ch√≠    Minh   l√†     trung  t√¢m    kinh   t·∫ø     l·ªõn    nh·∫•t   Vi·ªát   Nam    .     \nPhoBERT:   B-LOC  I-LOC  I-LOC  I-LOC  I-LOC  O      O      I-LOC  I-LOC  I-LOC  O      O      B-LOC  I-LOC  O     \nFinetuned: B-LOC  I-LOC  I-LOC  I-LOC  I-LOC  O      O      I-LOC  I-LOC  O      O      O      B-LOC  I-LOC  O     \n====================================================================================================\nTokens:    Ti·∫øn   sƒ©     L√™     Th·ªã    Thu    H√†     ƒëang   nghi√™n  c·ª©u    v·ªÅ     tr√≠    tu·ªá    nh√¢n   t·∫°o    .     \nPhoBERT:   O      O      B-PER  I-PER  I-PER  I-PER  O      O       O      O      O      O      O      O      O     \nFinetuned: O      O      B-PER  I-PER  I-PER  I-PER  O      O       O      O      O      O      O      O      O     \n====================================================================================================\nTokens:    T·∫≠p    ƒëo√†n   FPT    Software l√†     m·ªôt    trong  nh·ªØng  c√¥ng   ty     c√¥ng   ngh·ªá   h√†ng   ƒë·∫ßu    Vi·ªát   Nam    .     \nPhoBERT:   O      O      B-ORG  I-ORG   O      O      O      O      O      O      O      O      O      O      B-LOC  I-LOC  O     \nFinetuned: B-ORG  I-ORG  I-ORG  I-ORG   O      O      O      O      O      I-ORG  I-ORG  I-ORG  O      O      B-LOC  I-LOC  O     \n====================================================================================================\nTokens:    V·ªãnh   H·∫°     Long   ƒë∆∞·ª£c   UNESCO  c√¥ng   nh·∫≠n   l√†     Di     s·∫£n    Thi√™n  nhi√™n  Th·∫ø    gi·ªõi   .     \nPhoBERT:   B-LOC  I-LOC  I-LOC  O      B-ORG   O      O      O      B-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  O     \nFinetuned: B-LOC  I-LOC  I-LOC  O      B-ORG   O      O      O      O      O      I-ORG  I-ORG  I-ORG  I-ORG  O     \n====================================================================================================\nTokens:    C√¥ng   ty     c·ªï     ph·∫ßn   S·ªØa    Vi·ªát   Nam    Vinamilk l√†     nh√†    s·∫£n    xu·∫•t   s·ªØa    h√†ng   ƒë·∫ßu    Vi·ªát   Nam    .     \nPhoBERT:   O      O      I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  B-ORG   O      O      O      O      O      O      O      B-LOC  I-LOC  O     \nFinetuned: B-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG   O      O      O      O      O      O      O      B-LOC  I-LOC  O     \n====================================================================================================\nTokens:    S√¥ng   H·ªìng   ch·∫£y   qua    th·ªß    ƒë√¥     H√†     N·ªôi    c·ªßa    Vi·ªát   Nam    .     \nPhoBERT:   B-LOC  I-LOC  O      O      O      O      B-LOC  I-LOC  O      B-LOC  I-LOC  O     \nFinetuned: B-LOC  I-LOC  O      O      O      O      B-LOC  I-LOC  O      B-LOC  I-LOC  O     \n====================================================================================================\nTokens:    √îng    Nguy·ªÖn  Thanh  Long   t·ª´ng   l√†     B·ªô     tr∆∞·ªüng  B·ªô     Y      t·∫ø     .     \nPhoBERT:   O      B-PER   I-PER  I-PER  O      O      O      O       B-ORG  I-ORG  I-ORG  O     \nFinetuned: O      B-PER   I-PER  I-PER  O      O      O      O       B-ORG  I-ORG  I-ORG  O     \n====================================================================================================\nTokens:    C·∫ßu    C·∫ßn    Th∆°    n·ªëi    li·ªÅn   hai    b·ªù     s√¥ng   H·∫≠u    thu·ªôc  th√†nh  ph·ªë    C·∫ßn    Th∆°    .     \nPhoBERT:   B-ORG  I-ORG  I-ORG  O      O      O      O      I-LOC  I-LOC  O      B-LOC  I-LOC  I-LOC  I-LOC  O     \nFinetuned: B-LOC  I-LOC  I-LOC  O      O      O      I-LOC  I-LOC  I-LOC  O      B-LOC  I-LOC  I-LOC  I-LOC  O     \n====================================================================================================\nTokens:    T·ªïng   B√≠     th∆∞    T√¥     L√¢m    ƒë√£     c√≥     b√†i    ph√°t   bi·ªÉu   quan   tr·ªçng  .     \nPhoBERT:   B-ORG  I-ORG  I-ORG  I-ORG  I-ORG  O      O      O      O      O      O      O      O     \nFinetuned: B-PER  I-PER  I-PER  I-PER  I-PER  O      O      O      O      O      O      O      O     \n====================================================================================================\nTokens:    Li√™n   H·ª£p    Qu·ªëc   c√≥     tr·ª•    s·ªü     ch√≠nh  t·∫°i    New    York   ,      Hoa    K·ª≥     .     \nPhoBERT:   B-ORG  I-ORG  I-ORG  O      O      O      O      O      B-LOC  I-LOC  O      B-LOC  I-LOC  O     \nFinetuned: B-ORG  I-ORG  I-ORG  O      O      O      O      O      B-LOC  I-LOC  O      B-LOC  I-LOC  O     \n====================================================================================================\nTokens:    ƒë·ªïi    Fukui  (      th√†nh  ph·ªë    )     \nPhoBERT:   O      B-LOC  I-LOC  I-LOC  I-LOC  I-LOC \nFinetuned: O      B-LOC  O      O      O      O     \n====================================================================================================\nTokens:    Con    ƒë∆∞·ªùng  t∆°     l·ª•a   \nPhoBERT:   B-ORG  I-ORG  I-ORG  I-ORG \nFinetuned: B-LOC  I-LOC  I-LOC  I-LOC \n====================================================================================================\nTokens:    ƒë·ªïi    Ch·ªâ    th·ªã    v·ªÅ     h·∫°n    ch·∫ø    c√°c    ch·∫•t   nguy   hi·ªÉm  \nPhoBERT:   O      B-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG  I-ORG \nFinetuned: O      O      O      O      O      O      O      O      O      O     \n====================================================================================================\nTokens:    T·ªïng   th·ªëng  Hoa    K·ª≥    \nPhoBERT:   B-ORG  I-ORG  I-ORG  I-ORG \nFinetuned: O      O      B-LOC  I-LOC \n====================================================================================================\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"for tokens, l, l_ in zip(token_list, label_list, label_list_):\n    print(tokens)\n    print([label2id[i] for i in l_])\n    print([label2id[i] for i in l])\n    print(\"=\"*50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T14:39:18.337053Z","iopub.execute_input":"2025-07-01T14:39:18.337361Z","iopub.status.idle":"2025-07-01T14:39:18.343188Z","shell.execute_reply.started":"2025-07-01T14:39:18.337338Z","shell.execute_reply":"2025-07-01T14:39:18.342146Z"}},"outputs":[{"name":"stdout","text":"['<s>', 'Th√†nh', 'ph·ªë', 'H·ªì', 'Ch√≠', 'Minh', 'l√†', 'trung', 't√¢m', 'kinh', 't·∫ø', 'l·ªõn', 'nh·∫•t', 'Vi·ªát', 'Nam', '.', '</s>']\n[0, 5, 6, 6, 6, 6, 0, 0, 6, 6, 6, 0, 0, 5, 6, 0, 0]\n[0, 5, 6, 6, 6, 6, 0, 0, 6, 6, 0, 0, 0, 5, 6, 0, 0]\n==================================================\n['<s>', 'Ti·∫øn', 'sƒ©', 'L√™', 'Th·ªã', 'Thu', 'H√†', 'ƒëang', 'nghi√™n', 'c·ª©u', 'v·ªÅ', 'tr√≠', 'tu@@', '·ªá', 'nh√¢n', 't·∫°o', '.', '</s>']\n[0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n==================================================\n['<s>', 'T·∫≠p', 'ƒëo√†n', 'FPT', 'Software', 'l√†', 'm·ªôt', 'trong', 'nh·ªØng', 'c√¥ng', 'ty', 'c√¥ng', 'ngh·ªá', 'h√†ng', 'ƒë·∫ßu', 'Vi·ªát', 'Nam', '.', '</s>']\n[0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0]\n[0, 3, 4, 4, 4, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 5, 6, 0, 0]\n==================================================\n['<s>', 'V·ªãnh', 'H·∫°', 'Long', 'ƒë∆∞·ª£c', 'UNESCO', 'c√¥ng', 'nh·∫≠n', 'l√†', 'Di', 's·∫£n', 'Thi√™n', 'nhi√™n', 'Th·∫ø', 'gi·ªõi', '.', '</s>']\n[0, 5, 6, 6, 0, 3, 0, 0, 0, 3, 4, 4, 4, 4, 4, 0, 0]\n[0, 5, 6, 6, 0, 3, 0, 0, 0, 0, 0, 4, 4, 4, 4, 0, 0]\n==================================================\n['<s>', 'C√¥ng', 'ty', 'c·ªï', 'ph·∫ßn', 'S·ªØa', 'Vi·ªát', 'Nam', 'Vinamilk', 'l√†', 'nh√†', 's·∫£n', 'xu·∫•t', 's·ªØa', 'h√†ng', 'ƒë·∫ßu', 'Vi·ªát', 'Nam', '.', '</s>']\n[0, 0, 0, 4, 4, 4, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0]\n[4, 3, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 4]\n==================================================\n['<s>', 'S√¥ng', 'H·ªìng', 'ch·∫£y', 'qua', 'th·ªß', 'ƒë√¥', 'H√†', 'N·ªôi', 'c·ªßa', 'Vi·ªát', 'Nam', '.', '</s>']\n[0, 5, 6, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0]\n[0, 5, 6, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0]\n==================================================\n['<s>', '√îng', 'Nguy·ªÖn', 'Thanh', 'Long', 't·ª´ng', 'l√†', 'B·ªô', 'tr∆∞·ªüng', 'B·ªô', 'Y', 't·∫ø', '.', '</s>']\n[0, 0, 1, 2, 2, 0, 0, 0, 0, 3, 4, 4, 0, 0]\n[0, 0, 1, 2, 2, 0, 0, 0, 0, 3, 4, 4, 0, 0]\n==================================================\n['<s>', 'C·∫ßu', 'C·∫ßn', 'Th∆°', 'n·ªëi', 'li·ªÅn', 'hai', 'b·ªù', 's√¥ng', 'H·∫≠u', 'thu·ªôc', 'th√†nh', 'ph·ªë', 'C·∫ßn', 'Th∆°', '.', '</s>']\n[0, 3, 4, 4, 0, 0, 0, 0, 6, 6, 0, 5, 6, 6, 6, 0, 0]\n[5, 5, 6, 6, 0, 0, 0, 6, 6, 6, 0, 5, 6, 6, 6, 0, 5]\n==================================================\n['<s>', 'T·ªïng', 'B√≠', 'th∆∞', 'T√¥', 'L√¢m', 'ƒë√£', 'c√≥', 'b√†i', 'ph√°t', 'bi·ªÉu', 'quan', 'tr·ªçng', '.', '</s>']\n[0, 3, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 1, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n==================================================\n['<s>', 'Li√™n', 'H·ª£p', 'Qu·ªëc', 'c√≥', 'tr·ª•', 's·ªü', 'ch√≠nh', 't·∫°i', 'New', 'York', ',', 'Hoa', 'K·ª≥', '.', '</s>']\n[0, 3, 4, 4, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0]\n[0, 3, 4, 4, 0, 0, 0, 0, 0, 5, 6, 0, 5, 6, 0, 0]\n==================================================\n['<s>', 'ƒë·ªïi', 'Fuk@@', 'ui', '(', 'th√†nh', 'ph·ªë', ')', '</s>']\n[5, 0, 5, 6, 6, 6, 6, 6, 5]\n[0, 0, 5, 6, 0, 0, 0, 0, 0]\n==================================================\n['<s>', 'Con', 'ƒë∆∞·ªùng', 't∆°', 'l·ª•a', '</s>']\n[4, 3, 4, 4, 4, 4]\n[6, 5, 6, 6, 6, 6]\n==================================================\n['<s>', 'ƒë·ªïi', 'Ch·ªâ', 'th·ªã', 'v·ªÅ', 'h·∫°n', 'ch·∫ø', 'c√°c', 'ch·∫•t', 'nguy', 'hi·ªÉm', '</s>']\n[4, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n==================================================\n['<s>', 'T·ªïng', 'th@@', '·ªëng', 'Hoa', 'K·ª≥', '</s>']\n[4, 3, 4, 4, 4, 4, 4]\n[0, 0, 0, 0, 5, 6, 0]\n==================================================\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer.save_model(\"./phobert-ner/final-model\")\ntokenizer.save_pretrained(\"./phobert-ner/final-model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T08:23:31.417389Z","iopub.execute_input":"2025-07-01T08:23:31.417946Z","iopub.status.idle":"2025-07-01T08:23:33.000068Z","shell.execute_reply.started":"2025-07-01T08:23:31.417921Z","shell.execute_reply":"2025-07-01T08:23:32.999485Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('./phobert-ner/final-model/tokenizer_config.json',\n './phobert-ner/final-model/special_tokens_map.json',\n './phobert-ner/final-model/vocab.txt',\n './phobert-ner/final-model/bpe.codes',\n './phobert-ner/final-model/added_tokens.json')"},"metadata":{}}],"execution_count":11}]}